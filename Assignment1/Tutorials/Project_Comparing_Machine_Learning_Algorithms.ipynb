{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled0.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMTnPtnMc07IJOfTfjfAq8o"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKcQJoTv0oRd"
      },
      "source": [
        "This tutorial is based on:\n",
        "\n",
        "Comparing Machine Learning Algorithms on a single Dataset(Classification). \n",
        "https://medium.com/@vaibhavpaliwal/comparing-machine-learning-algorithms-on-a-single-dataset-classification-46ffc5d3f278\n",
        "\n",
        "This website contains more detailed background information! This notebook is only the code execution. \n",
        "**READ the steps and details on the website**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onavPFjGCS9D"
      },
      "source": [
        "Step1-: The first step is to import the necessary libraries for the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh8iVoKer7_t"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# import seaborn as sns\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKfYdq8TN1D8"
      },
      "source": [
        "Custom functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtoMQYh0N3MA"
      },
      "source": [
        "\n",
        "def plotMatrix(data):\n",
        "  fig, ax = plt.subplots()\n",
        "  # Using matshow here just because it sets the ticks up nicely. imshow is faster.\n",
        "  ax.matshow(data, cmap='viridis')\n",
        "  for (i, j), z in np.ndenumerate(data):\n",
        "     ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6rGYbPNCVob"
      },
      "source": [
        "Step2-: Now as we imported the necessary libraries let’s import the dataset in the form of CSV file using the pandas library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVK6FbKZ2G82"
      },
      "source": [
        "# dataset fetched from kaggle: https://www.kaggle.com/roustekbio/breast-cancer-csv\n",
        "\n",
        "# you should first fetch the dataset from github and upload the dataset to your jetson\n",
        "data = pd.read_csv(\"./breastCancer.csv\")\n",
        "\n",
        "\n",
        "## sklearn has built in datasets\n",
        "## Can you run this also on this wine dataset?\n",
        "\n",
        "# from sklearn.datasets import load_wine\n",
        "# data = load_wine()\n",
        "# data = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "# data.head()\n",
        "\n",
        "# import kaggle\n",
        "# kaggle.api.authenticate()\n",
        "# kaggle.api.dataset_download_files('breastCancer.csv', path='.', unzip=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSsVGPm-Catw"
      },
      "source": [
        "Step3-: Check for the missing data and preprocess it, we will also look at the data axes and attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KsOcZ1D7Hnl"
      },
      "source": [
        "data.replace('?',-99999, inplace=True)\n",
        "print(data.axes)\n",
        "print(data.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPrw19nCdqo"
      },
      "source": [
        "Step4-: In this step, we will randomly select one row and visualize its data, we will also look for the shape of data, which means the total number of instances and attributes. The highlighted output is the shape of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtMygSsmBUp-"
      },
      "source": [
        "print (data.loc[20])\n",
        "print (data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLIFc7rXCkmC"
      },
      "source": [
        "Step5-: Now we will describe our data, it means we will look at the value of the statistics for each attribute. The (describe) function of pandas lib Generates descriptive statistics that summarize the central tendency, dispersion, and shape of a dataset’s distribution, excluding NaN values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Oc728mBtcy"
      },
      "source": [
        "print(data.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sAWA1jgCur6"
      },
      "source": [
        "Step6-: Now we will do a graphical representation of our dataset, in which we will use (histogram) feature to visualize the graph of each attribute. A histogram is a representation of the distribution of data. This function calls matplotlib.pyplot.hist(), on each series in the Data Frame, resulting in one histogram per column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVo7W-3pB15Z"
      },
      "source": [
        "data.hist(figsize=(15,15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQH09zt9COiK"
      },
      "source": [
        "Step7-: We will plot the scatter matrix for our dataset, which is broadly used for the understanding correlation between attributes. A scatter plot matrix can be formed for a collection of variables where each of the variables will be plotted against each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdabbIVrBW0o"
      },
      "source": [
        "scatter_matrix(data, figsize=(15,15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb4dbL4JYSD-"
      },
      "source": [
        "# axes = scatter_matrix(df, alpha=0.5, diagonal='kde')\n",
        "# corr = data.values()\n",
        "# for i, j in zip(*plt.np.triu_indices_from(axes, k=1)):\n",
        "#     axes[i, j].annotate(\"%.3f\" %corr[i,j], (0.8, 0.8), xycoords='axes fraction', ha='center', va='center')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVOMNJ-bDGGZ"
      },
      "source": [
        "Step8-: In this step, we will plot the correlation matrix to see the correlation between attributes. This also helps us in determining which attributes have high correlation and then we can decide which attribute is important for us. In Python, the correlation values lie between (-1 and 1).\n",
        "\n",
        "There are two key components of a correlation value: 1. magnitude — The larger the magnitude (closer to 1 or -1), the stronger the correlation. 2. sign — If negative, there is an inverse correlation. If positive, there is a regular correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsqMnGNxDLhJ"
      },
      "source": [
        "corrmat = data.corr()\n",
        "\n",
        "# #using seaborn\n",
        "# plt.figure(figsize=(10,10))\n",
        "# sns.heatmap(corrmat, cmap='viridis', annot=True, linewidths=0.5,)\n",
        "\n",
        "# plt.imshow(data, cmap='hot')\n",
        "# plt.show()\n",
        "\n",
        "# #using matplotlib\n",
        "# plt.matshow(data.corr())\n",
        "# plt.show()\n",
        "\n",
        "#using pandas\n",
        "corrmat.style.background_gradient(cmap='viridis').set_precision(4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJvRdclbPDVT"
      },
      "source": [
        "plotMatrix(corrmat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgpnqfqcLeU5"
      },
      "source": [
        "Step9-: In this step, we will convert the columns in a list and then divide our data into two variables (X and y), where X is consisting of all attributes except (class and ID). In y variable, we will put target value which is our “class” attribute and then look for the shape of both variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKBxz1uXLgP9"
      },
      "source": [
        "columns = data.columns.to_list()\n",
        "\n",
        "columns = [c for c in columns if c not in [\"class\", \"id\"]]\n",
        "\n",
        "target = \"class\"\n",
        "\n",
        "X = data[columns]\n",
        "y = data[target]\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDiEE5RNNl1I"
      },
      "source": [
        "Step10-: Now look at any random row of X and y to check we are going well.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIMgY66_NlLP"
      },
      "source": [
        "print(X.loc[20])\n",
        "print(y.loc[20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il0nRIbwN0k4"
      },
      "source": [
        "Step11-: This step is very important as we will split our data into the training and testing to check the accuracy and for this, we will use (model selection) library. When you’re working on a model and want to train it, you obviously have a dataset. But after training, we have to test the model on some test dataset. To do this we will split the dataset into two sets, one for training and the other for testing; and you do this before you start training your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X7DrgmpN4G6"
      },
      "source": [
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, test_size=0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaP9B6KiOO4q"
      },
      "source": [
        "seed=5\n",
        "scoring = 'accuracy'\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zZiu744OsLL"
      },
      "source": [
        "Step12 -: Sometimes we get the future warning in our code, so to ignore them we will use the below command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC3xzlRnOtGq"
      },
      "source": [
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRCwL-0MO_mK"
      },
      "source": [
        "Step13 -: This is the most important step of our code, where we will import both algorithms (SVM and Random Forest) and then we will train model and test it using 10-fold cross-validation. Firstly, look at the code and output then we will discuss the features and parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvEuGty3PBr6"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "models = []\n",
        "\n",
        "models.append(('SVM', SVC(gamma='auto')))\n",
        "models.append(('RFC', RandomForestClassifier(max_depth=5, n_estimators=40)))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "for name, model in models:\n",
        "  kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
        "  cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "  print(cv_results)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s Algorithm: Accuracy %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlMqOmPwTk2m"
      },
      "source": [
        "Step14 -: Now we will plot an algorithm comparison box plot to compare the accuracy of both algorithms and as we can see the accuracy calculated by Random Forest is more than the accuracy of SVM. It means RF is more accurate than SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfFaKe1hTm3e"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHWv3rkyUY9b"
      },
      "source": [
        "Step15 -: Let’s visualize the result of all 10 folds graphically and look at the mean of all the scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiEn0ZrOUZrI"
      },
      "source": [
        "# Yellowbrick is not supported is Jetson (see next code block)\n",
        "\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# # from yellowbrick.model_selection import CVScores\n",
        "\n",
        "# _, ax = plt.subplots()\n",
        "\n",
        "# cv = StratifiedKFold(10)\n",
        "\n",
        "# oz = CVScores(RandomForestClassifier(max_depth=5, n_estimators=40), ax=ax, cv=cv, scoring='accuracy')\n",
        "# oz.fit(X,y)\n",
        "# oz.poof()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cAOW4KTAZAX"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "CVSCORE = cross_val_score(RandomForestClassifier(max_depth=5, n_estimators=40), X, y, cv=10)\n",
        "# print(CVSCORE)\n",
        "df = pd.DataFrame(CVSCORE)\n",
        "\n",
        "# cv = StratifiedKFold(10)\n",
        "ax = df.plot.bar()\n",
        "for p in ax.patches:\n",
        "  ax.annotate(str(round(p.get_height(),4)),(p.get_x(), p.get_height())  )\n",
        "print(\"Mean score: \", df.mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgIUgzAZVO8W"
      },
      "source": [
        "Step16 -: Now we will make predictions on the validation sheet, we will look at the accuracy score and classification report which is consisting of many important parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmSqKQ4-VPy8"
      },
      "source": [
        "for name, model in models:\n",
        "  model.fit(X_train, y_train)\n",
        "  predictions = model.predict(X_test)\n",
        "  print(name)\n",
        "  print(accuracy_score(y_test, predictions))\n",
        "  print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8ZIXgCCWsvm"
      },
      "source": [
        "Step17 -: Now we will look at the confusion matrix to evaluate the accuracy of classification. By definition a confusion matrix C is such that Ci,j is equal to the number of observations are known to be in the group I but predicted to be in group j. Thus, in binary classification, the count of true negatives is C0,0, false negatives are C1,0, true positives is C1,1 and false positives are C0,1. Normally a confusion matrix looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve4TsCLXWxPF"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "predict = model.predict(X_test)\n",
        "confusion = confusion_matrix(y_test,predict)\n",
        "print(\"==== Confusion Matrix ===\")\n",
        "print(confusion)\n",
        "print('\\n')\n",
        "\n",
        "#using matplotlib\n",
        "# plt.matshow(confusion)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# def plotMatrix(data):\n",
        "#   fig, ax = plt.subplots()\n",
        "#   # Using matshow here just because it sets the ticks up nicely. imshow is faster.\n",
        "#   ax.matshow(data, cmap='viridis')\n",
        "#   for (i, j), z in np.ndenumerate(data):\n",
        "#      ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
        "#   plt.show()\n",
        "\n",
        "# plotMatrix(confusion)\n",
        "\n",
        "# #using seaborn\n",
        "# from sklearn import metrics\n",
        "# cnf_matrix = metrics.confusion_matrix(y_test, predict)\n",
        "# p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True)\n",
        "\n",
        "\n",
        "\n",
        "#using pandas\n",
        "pd.DataFrame(confusion).style.background_gradient(cmap='viridis').set_precision(4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5-48wyJcEUr"
      },
      "source": [
        "Step18 -: In this step we will calculate the Cohen Kappa score and Matthews Correlation Coefficient (MCC)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpxQZ2oncEzR"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "cohen_score = cohen_kappa_score(y_test, predictions)\n",
        "print(\"kappa score: \", cohen_score)\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "MCC = matthews_corrcoef(y_test, predictions)\n",
        "print(\"MCC score: \", MCC)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}